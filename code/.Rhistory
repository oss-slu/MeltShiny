stringsAsFactors = FALSE,
School = c("INTEGRATED DESIGN ELECTRONICS ACADEMY IDEA PCS",
"SCHOOL FOR EDUCATIONAL EVOLUTION AND DEVELOPMENT (SEED) PCS",
"OPTIONS PCS","BOOKER T  WASHINGTON PCS",
"CESAR CHAVEZ CAPITOL HILL PCS",
"WASHINGTON METROPOLITAN HS","MAYA ANGELOU PCS-EVANS CAMPUS HS",
"WASHINGTON MATH SCIENCE TECH PCS",
"PHELPS ARCHITECTURE CONSTRUCTION AND ENGINEERING HS",
"SPINGARN STAY","HYDE LEADERSHIP PCS",
"ELLINGTON SCHOOL OF THE ARTS","TUITION GRANT",
"SCHOOL WITHOUT WALLS SHS","BALLOU STAY","LUKE MOORE ALTERNATIVE HS",
"SPINGARN HS","MCKINLEY TECHNOLOGY HS","COLUMBIA HEIGHTS EC",
"CARDOZO HS AT MEYER","COOLIDGE HS","DUNBAR HS","WOODSON H D HS",
"FRIENDSHIP PCS WOODSON COLLEGIATE ACADEMY","ROOSEVELT HS",
"ANACOSTIA HS","WILSON HS","BALLOU HS"),
Total_Male = c(26L,28L,49L,33L,39L,47L,38L,29L,47L,57L,47L,38L,82L,
39L,69L,77L,100L,91L,93L,114L,129L,119L,124L,112L,152L,
172L,195L,210L),
Number_Male = c(20L,17L,27L,24L,26L,13L,11L,25L,42L,4L,33L,36L,1L,37L,
6L,22L,50L,80L,49L,39L,68L,61L,51L,97L,64L,48L,135L,89L),
Percent_Male = c(77L,61L,55L,73L,67L,28L,29L,86L,89L,7L,70L,95L,1L,
95L,9L,29L,50L,88L,53L,34L,53L,51L,41L,87L,42L,28L,69L,
42L),
Total_Female = c(39L,27L,25L,42L,39L,39L,51L,64L,46L,41L,54L,67L,31L,
81L,84L,88L,77L,90L,108L,102L,91L,120L,130L,156L,120L,
173L,189L,174L),
Number_Female = c(30L,25L,14L,31L,24L,16L,24L,56L,42L,3L,43L,65L,1L,73L,
7L,37L,42L,86L,82L,51L,62L,80L,84L,145L,67L,89L,144L,
103L),
Percent_Female = c(77L,93L,56L,74L,62L,41L,47L,88L,91L,7L,80L,97L,3L,
90L,8L,42L,55L,96L,76L,50L,68L,67L,65L,93L,56L,51L,
76L,59L)
)
# Converting data to longitudinal format
longitudinal_data <- data %>%
pivot_longer(cols = -School, names_to = c(".value", "Gender"),
names_sep = "_", values_to = c("Total", "Number", "Percent"))
print(longitudinal_data)
# displaying statistical summary based on gender
gendersummary <- long_data %>%
group_by(Gender) %>%
summarise(
AverageTotal = mean(Total, na.rm = TRUE),
MedianTotal = median(Total, na.rm = TRUE),
StandardDeviationTotal = sd(Total, na.rm = TRUE),
AverageNumber = mean(Number, na.rm = TRUE),
MedianNumber = median(Number, na.rm = TRUE),
StandardDeviationNumber = sd(Number, na.rm = TRUE),
AveragePercent = mean(Percent, na.rm = TRUE),
MedianPercent = median(Percent, na.rm = TRUE),
StandardDeviationPercent = sd(Percent, na.rm = TRUE)
)
print(gendersummary)
# Making box plot for genders
box_plot_considering_genders <- ggplot(long_data, aes(x = Gender, y = Total, fill = Gender)) +
geom_boxplot() +
labs(title = "Box Plot of Total by Gender",
x = "Gender",
y = "Total") +
theme_minimal() +
scale_fill_manual(values = c("Male" = "blue", "Female" = "pink", "Other" = "green"))
plot(box_plot_considering_genders)
# Making box plot without considering gender
box_plot_without_considering_gender <- ggplot(data, aes(x = "", y = Total_Male + Total_Female, fill = "Total")) +
geom_boxplot() +
labs(title = "Box Plot without Considering Gender",
x = NULL, y = "Total Numbers") +
theme_minimal() +
theme(legend.position = "none") +
scale_fill_manual(values = c("Male" = "blue", "Female" = "pink", "Other" = "green"))
# Making box plot without considering gender
box_plot_without_considering_gender <- ggplot(data, aes(x = "", y = Total_Male + Total_Female, fill = "Total")) +
geom_boxplot() +
labs(title = "Box Plot without Considering Gender",
x = NULL, y = "Total Numbers") +
theme_minimal() +
theme(legend.position = "none")
print(box_plot_without_considering_gender)
# Making box plot without considering gender
box_plot_without_considering_gender <- ggplot(data, aes(x = "", y = Total_Male + Total_Female, fill = "Total")) +
geom_boxplot() +
labs(title = "Box Plot without Considering Gender",
x = NULL, y = "Total Numbers") +
theme_minimal() +
theme(legend.position = "none") +
scale_fill_manual(values = c("Total" = "blue"))
print(box_plot_without_considering_gender)
print(gendersummary)
# Load necessary libraries
library(ggplot2)
library(readxl)
library(openxlsx)
data <- read_excel("D://Pranitha//SLU//DB//New_folder//formatted_cleaned_carsales_1.xlsx")
# Step 2: Understand the structure of the dataset
str(data)  # This function will show the structure of the dataframe including data types and sample data for each column
# Get a quick look at the first few and last few rows of the dataset
head(data)  # Shows the first 6 rows by default
tail(data)  # Shows the last 6 rows by default
# Summary statistics for numerical and categorical data
summary(data)
# Optionally, for large datasets, you might want to see a random sample
set.seed(123)  # Setting seed for reproducibility
sample_n(data, 10)
# Analyzing price by condition
price_by_condition <- data %>%
group_by(condition_type) %>%
summarise(Average_Price = mean(sellingprice, na.rm = TRUE))
# Load necessary libraries
library(ggplot2)
library(readxl)
library(openxlsx)
data <- read_excel("D://Pranitha//SLU//DB//New_folder//formatted_cleaned_carsales_1.xlsx")
set.seed(123)  # Setting seed for reproducibility
sample_n(data, 10)
library(dplyr)
# Analyzing price by condition
price_by_condition <- data %>%
group_by(condition_type) %>%
summarise(Average_Price = mean(sellingprice, na.rm = TRUE))
print(price_by_condition)
# Plotting prices by car condition
ggplot(price_by_condition, aes(x = condition_type, y = Average_Price, fill = condition_type)) +
geom_bar(stat = "identity") +
labs(title = "Average Selling Price by Car Condition", x = "Condition Type", y = "Average Selling Price")
# Counting sales by hour of the day
sales_volume_by_hour <- data %>%
group_by(sale_hour) %>%
summarise(Sales_Count = n())
print(sales_volume_by_hour)
# Plotting sales volume by hour
ggplot(sales_volume_by_hour, aes(x = sale_hour, y = Sales_Count)) +
geom_line() +
labs(title = "Sales Volume by Hour of the Day", x = "Hour", y = "Number of Sales")
# Grouping by odometer use category
price_by_odometer_use <- data %>%
group_by(odometer_use) %>%
summarise(Average_Price = mean(sellingprice, na.rm = TRUE))
print(price_by_odometer_use)
# Plotting
ggplot(price_by_odometer_use, aes(x = odometer_use, y = Average_Price, fill = odometer_use)) +
geom_bar(stat = "identity") +
labs(title = "Average Selling Price by Odometer Use", x = "Odometer Use Category", y = "Average Selling Price")
# Identifying top 10 popular models
popular_models <- data %>%
count(model, sort = TRUE) %>%
top_n(10)
print(popular_models)
# Visualizing the top models
ggplot(popular_models, aes(x = reorder(model, n), y = n)) +
geom_bar(stat = "identity", fill = "blue") +
labs(title = "Top 10 Popular Car Models", x = "Model", y = "Number of Cars Sold") +
coord_flip()  # This makes it easier to read the model names
# Calculating average selling price by month
monthly_sales_trends <- data %>%
group_by(sale_month) %>%
summarise(Count = n(), Average_Price = mean(sellingprice, na.rm = TRUE))
print(monthly_sales_trends)
# Plotting sales count and price trends over the months
ggplot(monthly_sales_trends, aes(x = sale_month)) +
geom_line(aes(y = Count, group = 1, colour = "Sales Count")) +
geom_line(aes(y = Average_Price, group = 1, colour = "Average Price")) +
labs(title = "Car Sales Trends and Average Prices by Month", x = "Month", y = "Count / Average Price") +
scale_colour_manual("", values = c("Sales Count" = "blue", "Average Price" = "red"))
# Analyzing price by condition and hour
price_by_condition_hour <- data %>%
group_by(condition_type, sale_hour) %>%
summarise(Average_Price = mean(sellingprice, na.rm = TRUE))
print(price_by_condition_hour)
# Visualization
ggplot(price_by_condition_hour, aes(x = sale_hour, y = Average_Price, colour = condition_type)) +
geom_line() +
labs(title = "Average Selling Price by Condition and Sale Hour", x = "Hour of Sale", y = "Average Selling Price")
#Strategy 1 : . Prioritize and Market High-Value Segments
ggplot(data, aes(x = condition_type, y = sellingprice, fill = condition_type)) +
geom_boxplot() +
labs(title = "Price Distribution by Car Condition", x = "Condition Type", y = "Selling Price")
#Strategy 2: Target Sales During Peak Hours
ggplot(sales_volume_by_hour, aes(x = sale_hour, y = Sales_Count)) +
geom_line(group = 1, color = "red") +
labs(title = "Sales Volume by Hour", x = "Hour of Sale", y = "Sales Count")
#Strategy 3:  Leverage Odometer Readings for Pricing
ggplot(data, aes(x = odometer_use, y = sellingprice, fill = odometer_use)) +
geom_bar(stat = "identity") +
labs(title = "Average Selling Price by Odometer Use", x = "Odometer Usage", y = "Average Price")
#Strategy 4: Exploit Popularity of Models
ggplot(popular_models, aes(x = reorder(model, -n), y = n, fill = model)) +
geom_col() +
labs(title = "Top 10 Most Popular Car Models", x = "Model", y = "Number of Units Sold") +
coord_flip()
# Strategy 5: Seasonal Marketing and Stocking Strategy
ggplot(monthly_sales_trends, aes(x = sale_month)) +
geom_line(aes(y = Average_Price, color = "Average Price")) +
labs(title = "Monthly Sales Trends", x = "Month", y = "Average Price")
#Strategy 6: Adjust Pricing Based on Time of Sale
ggplot(price_by_condition_hour, aes(x = sale_hour, y = Average_Price, color = condition_type)) +
geom_line() +
facet_wrap(~condition_type) +
labs(title = "Price Variability by Hour and Condition", x = "Hour of Sale", y = "Average Selling Price")
#Strategy 1 : . Prioritize and Market High-Value Segments
ggplot(data, aes(x = condition_type, y = sellingprice, fill = condition_type)) +
geom_boxplot() +
labs(title = "Price Distribution by Car Condition", x = "Condition Type", y = "Selling Price")
#Strategy 2: Target Sales During Peak Hours
ggplot(sales_volume_by_hour, aes(x = sale_hour, y = Sales_Count)) +
geom_line(group = 1, color = "red") +
labs(title = "Sales Volume by Hour", x = "Hour of Sale", y = "Sales Count")
#Strategy 3:  Leverage Odometer Readings for Pricing
ggplot(data, aes(x = odometer_use, y = sellingprice, fill = odometer_use)) +
geom_bar(stat = "identity") +
labs(title = "Average Selling Price by Odometer Use", x = "Odometer Usage", y = "Average Price")
#Strategy 4: Exploit Popularity of Models
ggplot(popular_models, aes(x = reorder(model, -n), y = n, fill = model)) +
geom_col() +
labs(title = "Top 10 Most Popular Car Models", x = "Model", y = "Number of Units Sold") +
coord_flip()
# Strategy 5: Seasonal Marketing and Stocking Strategy
ggplot(monthly_sales_trends, aes(x = sale_month)) +
geom_line(aes(y = Average_Price, color = "Average Price")) +
labs(title = "Monthly Sales Trends", x = "Month", y = "Average Price")
#Strategy 6: Adjust Pricing Based on Time of Sale
ggplot(price_by_condition_hour, aes(x = sale_hour, y = Average_Price, color = condition_type)) +
geom_line() +
facet_wrap(~condition_type) +
labs(title = "Price Variability by Hour and Condition", x = "Hour of Sale", y = "Average Selling Price")
data <- read.csv("D:/Pranitha/SLU/Predictive_Modelling/Alzheimer.csv")
#1- Fine the Linear Regression Coefficients
fit1=lm(sqft ~ price+beds ,data=data)
#2 -Plot the actual data and Decomposition graph
data1<-t(data)
ts<-ts(data1, frequency=1000)
ts_compo<-decompose(ts)
data1<-t(data)
ts<-ts(data1, frequency=1000)
ts_compo<-decompose(ts)
acf(data$Data_Value, pl=FALSE)
library(astsa)
diff6=diff(data$Data_Value, 6)
diffland6=diff(diff6,1)
acf2(diffland6,1)
dtb=residuals(lm(diff~time(diff6)))
library(astsa)
diff6=diff(data$Data_Value, 6)
diff1and6=diff(diff6,1)
acf2(diff1and6,1)
dtb=residuals(lm(diff6~time(diff6)))
acf2(dtb)
sarima(dtb,1,0,0,0,1,1,6)
ar(data$Data_Value, aic = TRUE, order.max = NULL, method = c("yule-walker", "burg", ))
library(FinTS)
data.archTest<- ArchTest(data$Data_Value, lags=2, demean=TRUE)
data.archTest
library(fGarch)
arch.fit<- garchFit(~garch(1,0),data=data$Data_Value, trace=FALSE)
library(fGarch)
arch.fit<- garchFit(~garch(1,0),data=data$Data_Value, trace=F)
xtabs(~Data_Value+Year_End+LocationID, data=data)
xtabs(~ Data_Value+YearEnd+LocationID, data=data)
data$rank <-factor(data$rank)
mylogit<- glm(High_Confidence_Limit ~ Data_Value+YearEnd+LocationID, data=data,family="binomial")
#1- Fine the Linear Regression Coefficients
fit1=lm(High_Confidence_Limit ~ Data_Value+YearEnd+LocationID ,data=data)
install.packages("devtools")
library('devtools')
data1 <- read.csv("D:/Pranitha/SLU/Predictive_Modelling/Alzheimer.csv")
data<- data1[1:90,]
#1- Fine the Linear Regression Coefficients
fit1=lm(High_Confidence_Limit ~ Data_Value+YearEnd+LocationID ,data=data)
summary(fit1)
plot(fit1)
plot(timeseriescomponents)
library(astsa)
ts.plot(data$Data_Value,col='yellow')
fitclose<- HoltWinters(timeseries)
plot(fitclose, col='blue')
data1 <- read.csv("D:/Pranitha/SLU/Predictive_Modelling/Alzheimer.csv")
data<- data1[1:90,]
#1- Fine the Linear Regression Coefficients
fit1=lm(High_Confidence_Limit ~ Data_Value+YearEnd+LocationID ,data=data)
summary(fit1)
plot(fit1)
#LocationID  -1.519e-04  9.853e-05  -1.542    0.127
#Based on LR, data Value has more effect on awareness of person than location
#2 -Plot the actual data and Decomposition graph
par(mar=c(1,1,1,1))
pairs(~ YearEnd+LocationID+Data_Value+Data_Value_Alt, data=data)
#seasonal Decomposition
timeseries<- ts(data$Data_Value, frequency =12)
timeseriescomponents<-decompose(timeseries)
plot(timeseriescomponents)
#fitting line
library(astsa)
ts.plot(data$Data_Value,col='yellow')
fitclose<- HoltWinters(timeseries)
plot(fitclose, col='blue')
#forecast
forecast(fitclose,10)
#forecast
library(forecast)
forecast(fitclose,10)
#Aug 8       18.08506 -13.983356 50.15347 -30.959348  67.12946
#Sep 8       23.90778  -8.163943 55.97949 -25.141685  72.95724
#Oct 8       38.64864   6.571101 70.72618 -10.409722  87.70700
#Nov 8       15.05135 -17.035222 47.13792 -34.020828  64.12353
#Dec 8       37.53392   5.434390 69.63345 -11.558074  86.62591
#Jan 9       41.76375   9.646652 73.88085  -7.355114  90.88262
#Feb 9       26.87561  -5.264378 59.01560 -22.278261  76.02948
#Mar 9       11.15695 -21.011932 43.32584 -38.041111  60.35501
#Apr 9       15.39920 -16.805276 47.60367 -33.853294  64.65169
#3- Find and plot the acf.
acf(data$Data_Value, pl=FALSE)
acf(data$Data_Value, lag.max = 20, type = c("correlation", "covariance", "partial"),plot = TRUE)
#lag is 2
#4- Find Auto-regression Coefficients.
ar(data$Data_Value, aic = TRUE, order.max = NULL, method = c("yule-walker", "burg","ols","mle","yw" ))
AR <- ar(data$Data_Value, aic=TRUE, order.max = NULL, method ="burg")
AR <- ar(data$Data_Value, aic=TRUE, order.max = NULL, method ="burg")
AR$ar
AR
#there is no autocorrelation
#5- ARIMA.
library(astsa)
diff6=diff(data$Data_Value, 6)
diff1anda6=diff(diff6,1)
acf2(diff1and6,1)
dtb=residuals(lm(diff6~time(diff6)))
acf2(dtb)
sarima(dtb,1,0,0,0,1,1,6)
#high randomness, lag~3, p_value<0,05then reject Null Hypothesis
# Model is effective, accuracy=80%
#6-GARCH
library(FinTS)
data.archTest<- ArchTest(data$Data_Value, lags=2, demean=TRUE)
data.archTest
#P value >0.05 accept NH there is no arch effect
library(fGarch)
arch.fit<- garchFit(~garch(1,0),data=data$Data_Value, trace=F)
summary(arch.fit)
acf(data$Data_Value, lag.max = 20, type = c("correlation", "covariance", "partial"),plot = TRUE)
acf(data$Data_Value, lag.max = 20, type = c("correlation", "covariance", "partial"),plot = FALSE)
AR
#[,1]  [,2] [,3] [,4] [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18]
#ACF  -0.23  0.04 0.02 0.00 0.29 -0.59  0.17 -0.03 -0.07 -0.07 -0.27  0.15 -0.15 -0.02 -0.02  0.06  0.18 -0.15
#PACF -0.23 -0.01 0.03 0.01 0.30 -0.53 -0.03  0.00 -0.09 -0.15 -0.06 -0.33 -0.12 -0.04 -0.13 -0.01  0.08 -0.33
#[,19] [,20]
#ACF   0.14  0.00
#PACF -0.03 -0.08
sarima(dtb,1,0,0,0,1,1,6)
library(FinTS)
data.archTest<- ArchTest(data$Data_Value, lags=2, demean=TRUE)
data.archTest
library(fGarch)
arch.fit<- garchFit(~garch(1,0),data=data$Data_Value, trace=F)
summary(arch.fit)
install.packages("BiocManager")
BiocManager::install("multtest")
# Assuming you have the golub data already loaded in R
# Find the index of the gene named "Paxillin mRNA"
paxillin_index <- grep("Paxillin mRNA", golub.gnames[, 2])
# Install Bioconductor if not installed
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
# Install the golubEsets package from Bioconductor
BiocManager::install("golubEsets")
# Find the index of the gene named "Paxillin mRNA"
paxillin_index <- grep("Paxillin mRNA", golub.gnames[, 2])
#(a) Compute the effect size for "Paxillin mRNA" gene
# Load the golubEsets package
library(golubEsets)
# Load the Golub dataset
data(Golub_Merge)
# Check the structure of the dataset
str(Golub_Merge)
# Assuming you have the golub data already loaded in R
# Find the index of the gene named "Paxillin mRNA"
paxillin_index <- grep("Paxillin mRNA", golub.gnames[, 2])
library(multtest)
# Load the golub data
data(golub)
# Check the structure of the golub dataset
str(golub)
#(a) Compute the effect size for "Paxillin mRNA" gene
# Load the golubEsets package
library(golubEsets)
# Load the Golub dataset
data(Golub_Merge)
# Check the structure of the dataset
str(Golub_Merge)
# Find the index of the gene named "Paxillin mRNA"
paxillin_index <- grep("Paxillin mRNA", golub.gnames[, 2])
# Extract the expression values for the "Paxillin mRNA" gene
paxillin_expression <- golub[paxillin_index, ]
# Compute the sample mean and standard deviation
mean_paxillin <- mean(paxillin_expression)
sd_paxillin <- sd(paxillin_expression)
# Calculate the effect size
effect_size_paxillin <- mean_paxillin / sd_paxillin
effect_size_paxillin
install.packages("BiocManager")
install.packages("BiocManager")
BiocManager::install("multtest")
library(multtest)
# Load the golub data
data(golub)
# Check the structure of the golub dataset
str(golub)
# Find the index of the gene named "Paxillin mRNA"
paxillin_index <- grep("Paxillin mRNA", golub.gnames[, 2])
# Extract the expression values for the "Paxillin mRNA" gene
paxillin_expression <- golub[paxillin_index, ]
# Compute the sample mean and standard deviation
mean_paxillin <- mean(paxillin_expression)
sd_paxillin <- sd(paxillin_expression)
# Calculate the effect size
effect_size_paxillin <- mean_paxillin / sd_paxillin
effect_size_paxillin
#(b) Compute effect sizes for the first 2500 genes using ALL patients
# Subset the data for ALL patients (ALL patients are in columns where golub.cl == 0)
all_patients <- golub[, golub.cl == 0]
# Initialize an empty vector to store effect sizes
effect_sizes <- numeric(2500)
# Loop through the first 2500 genes and compute the effect sizes
for (i in 1:2500) {
gene_expression <- all_patients[i, ]
effect_sizes[i] <- mean(gene_expression) / sd(gene_expression)
}
# Create a histogram of the effect sizes
hist(effect_sizes, main="Histogram of Effect Sizes for First 2500 Genes",
xlab="Effect Size", col="lightblue", border="black")
# Find the indices of the 4 largest effect sizes
top_4_indices <- order(effect_sizes, decreasing = TRUE)[1:4]
# Extract the gene names for the top 4 effect sizes
top_4_genes <- golub.gnames[top_4_indices, 2]
# Display the 4 largest effect sizes and their corresponding gene names
top_4_effect_sizes <- effect_sizes[top_4_indices]
top_4_results <- data.frame(Gene = top_4_genes, Effect_Size = top_4_effect_sizes)
top_4_results
# Install Bioconductor (if not already installed)
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
# Install the ALL package from Bioconductor
BiocManager::install("ALL")
# Load the ALL package and dataset
library(ALL)
data(ALL)
str(ALL)
#(a) Create a Frequency Table and Pie Chart for Disease Type/Stage
disease_stage_table <- table(ALL$BT)
print(disease_stage_table)
pie(disease_stage_table, main="Disease Type/Stage of Leukemia Patients",
col=rainbow(length(disease_stage_table)), labels=names(disease_stage_table))
pie(disease_stage_table, main="Disease Type/Stage of Leukemia Patients",
col=rainbow(length(disease_stage_table)), labels=names(disease_stage_table))
# Extract gene expressions for patients in disease stage T3
t3_gene_expressions <- exprs(ALL[, ALL$BT == "T3"])
# Create a box plot for each patient/column in disease stage T3
boxplot(t3_gene_expressions, main="Gene Expressions for Patients in Disease Stage T3",
xlab="Patients", ylab="Gene Expression", col="lightblue",
names=colnames(t3_gene_expressions))
#(c) Compute the Mean Gene Expression for Each Patient in Disease Stage T3
# Compute the mean gene expression for each patient in disease stage T3
mean_gene_expressions <- colMeans(t3_gene_expressions)
# View the means
print(mean_gene_expressions)
setwd("D:/New Volume/Pranitha/SLU/MeltShiny/code")
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
